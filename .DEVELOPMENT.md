# Development Notes
Things that I don't necessarily want to go into the README.md but I don't mind
being included with the project. 


## Brainstorming Stealth
Research, concepts, and ideas relating to hiding the use of explicitly **Webdriver**
ideally; using every strategy possible, broken up into three primary categories
(once we determine what existing higher-level "crawler" tool like **"Rod"**, or
**"Playwright"**, or **"Colly"**. 

Our development should focus on what makes this project unique, to maximize our
time volunteering towards this common goal: *unhindered browser automation*.

  * Don't use the standard port *4444*, this could be checked with JavaScript
    running in the browser. Ideally, switch to using a socket connection, or
    some other IPC methodology.

  * Conceal CDC JavaScript added to each page better than just renaming it,
    especially concealing it in the exact format (for now, while we are binary
    patching, we can use 0's because we have fixed allocated memory space to get
    a variable length name). 

    Ideally, if we can, use another technique entirely to introduce JavaScript 
    actions to a loaded page, because a large portion of the strategy to 
    identify automated browsers is by the very loud, and obvious tools used to
    control the browser that are standardized.

    If we have to maybe even input JavaScript via the console since we are 
    intentionally working with full browser implementations. 

    Alternatively, an extension could be used to introduce JavaScript, taking
    a popular extension and modifying it to introduce our JavaScript steathily.

  * Switch off **Chromedriver**, or at work on patching the source code and
    compiling it, enabling much fine grain control over the tool. 

    However, eventually we want to be supporting **Webdriver**, which is more
    generic and works on the top 5 browsers. 

    A single correct implementation can interact with **Firefox**, and 
    **Chrome**; and this should be our aim.

  * Transparent proxy around our browser to modify incoming JavaScript, and
    outgoing data. In addition, increasing the speed of our implementation by
    caching common libraries, frameworks, or other assets. Also, simply reject
    connections to certain hosts, and monitor exfiltrated data which will
    provide us with a way to determine exactly what the third-party companies
    offering "bot fingerprinting" are sending home.
    
    Providing a cheatsheet on what is expected. Possibly even by modifying 
    outgoing data to fit expectations rather than changing the browser. 

  * Most likely will need to patch the Browsers directly, either binary or
    source patching to conceal automation, and hide whatever clues browser
    developers may have seeded so they can determine if the browser is
    automated. 

    Ideally, we bypass needing to do this; but it may end up being an
    inevitability, as it would give us the control we need to "blend-in" into
    the rest of internet traffic or at least get close enough that blocking us,
    will inevitably block regular users, which will result in: **gg**.



  * 
